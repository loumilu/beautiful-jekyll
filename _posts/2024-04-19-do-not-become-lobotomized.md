---
layout: post
title: Do not become lobotomized
tags: [AI, Programming, Philosophy]
comments: true
mathjax: true
author: mintsuku
social-share: false
---
{: .box-note}
I am tired of hearing the same bullshit on Reddit and other forums that gobble the balls of AI.

Should I use AI to help me learn to program? 

No. That is my honest-to-God take, just no. The way most people use AI to "learn" is by copying and pasting and then asking it to "explain the code" to them. This is not learning. This is not programming. You are cheating yourself (and myself at one point) out of what it means to program. The fact of the matter is: Most people are lazy and aren't using AI in the good-faith way they'd love to portray it to you. Programming is literally problem-solving; you aren't doing any work by reading the solution from ChatGPT or Claude and "knowing" how it works. Let's not lie to ourselves. From personal experience, you are fucking yourself over BIG time. I am not saying do not use AI, but the idea that you are getting the same experience or making the same connections asking AI for a solution, as you are sitting, thinking, and researching about your problems, is madness, and you know that shit isn't the same.

I am literally a junior developer myself, which means I have not the slightest clue what I am even talking about, however I am speaking from experience when I say over-reliance on AI for programming and "learning" is a dangerous route to go down. A lot of senior developers will say, "Well, it doesn't output good code," and that is true; the code it produces isn't very good. But it's good enough (and most importantly, it works) to fool a junior developer (such as myself) and lead them down the wrong path. The last thing we want is a future full of handicapped devs who can't think for themselves. This isn't even just limited to software engineering but that's what this post is will focus on.


### You think you are learning

You aren't learning shit, full stop. Getting a perfectly tailored solution ready-made by a crappy chatbot IS NOT LEARNING, nor is asking it to explain how the code works to you "learning". 

Programming is problem solving at its core, programming teaches you HOW to think, how can you learn to think and design applications when someone else is doing it for you? I had the courtesy of seeing firsthand how much I was "learning" from AI. I was caught up in a pretty awkward situation: 

I was in the middle of getting my feet wet with programming when the first iteration of ChatGPT released. I had a bit of knowledge and concepts under my belt, enough to prompt the chatbot for Python code, which came out looking like spaghetti but I didn't know or care about that at the time and was just happy the chatbot was outputting useful stuff. Nothing changed until over the course of a few months later when I noticed I was more irritable and "fuck this shit" oriented, I couldn't think of solutions to programming problems I had and my cognitive battery drained quickly and I couldn't sustain a mode of focused thinking as long as I used to. 

The way I used to use ChatGPT was as follows:

- Run into an issue.
- Consult Lord GPT.
- Get a solution (hopefully)
- Repeat.

The first problem with this approach is that it trained my brain to never actually dive deeper into the code to figure out why the problem occurred and hash it out on my own. Instead, my brain said, "Don't worry, you don't need to do any uncomfortable thinking, just give it to ChatGPT he will figure it out". I never actually thought the problem through before asking for help, developing good problem-solving skills, which I believe is very important. 

There's a reason the elitist cunts on StackOverflow tell you to list all the things you've tried. If you haven't put any of your own effort into solving the problem, how can you expect to solve it and develop yourself as a software engineer? 


Solutions won't be handed to you, answers won't be handed to you. You need to do the heavy lifting. I learned this the hard way.

So here I am really only engaging in shallow thinking, and never exercising my brain's problem-solving abilities (which again, is literally what programming is about). If you are going to use a shitty chatbot to help you solve your problem, at least think about it a bit before you use it.

Bu- Bu- But how is this any different from using StackOverFlow?

The second problem with this approach is that I was never coming up with my own solutions, or adapting solutions to fit my needs. This is a big one. As far as I know, people do not come up with novel solutions to a problem, it has probably been solved time and time again. But they do come up with novel solutions to THEIR problem. This is another issue I take with AI. This isn't like StackOverflow, where you may find a (mostly correct and relevant) solution to your problem but still need to engineer it to fit your codebase most of the time, which requires actual thinking to be done. 

What an AI gives you is an almost perfectly tailored solution for your particular codebase, this removes any kind of thinking needed to be done to integrate with your codebase. So first you are mentally offloading the solution, **AND** the integration into your codebase? "What exactly are you doing?" is what I asked myself after I realized I wasn't really programming. More so being a copy-paste monkey, I was a code gluer and a code "reader". Not a programmer. I tried typing out the solutions myself, but that didn't help the fact that I wasn't doing any **thinking**.

Problem-solving is a process, a process that starts with encountering a problem and ending with either the solution for the problem or a plan to tackle the problem systematically. You aren't doing this sort of abstract thinking when you ask ChatGPT for code. So even if you understand the code, you don't understand why that code was used, how the components of that code interact to solve the problem, and even if you know this, the process of being stuck, and coming up with the solution, and implementing it has already been shown to create new [neural pathways](https://en.wikipedia.org/wiki/Neuroplasticity). How will you become better at problem-solving, and in turn, programming if you do not **try** and solve the damn problems yourself?

Let's make no mistake: Asking for help is NOT a bad thing, I used to think it was but I've come to realize it isn't. But before asking for help you need to try on your own first.


The third problem with this approach is that ChatGPT, Claude or any of these other motherfuckers simply aren't very good at programming. The code you are copying and pasting in your codebase right now will lead to technical debt down the line that you will have to deal with. 

{: .box-note}
"How is this any different from me, having to handle my own technical debt down the line".

Well, there isn't much difference. But do you want to handle an LLM's technical debt, or yours? I'd prefer to handle my shit later on than to handle ChatGPT's shit. 

If the solution that ChatGPT gave me didn't work, I got frustrated and went down what I like to call "prompt void". Where ChatGPT or any other AI gives you the wrong code and this is what your conversation looks like:

- **ME:** This didn't fix the error
  - **GPT:** You are right; my apologies for the oversight. To fix this issue you must...
- **ME:** Issue still persists, It's telling me that...
  - **GPT:** Sorry for the misunderstanding, to fix this issue...
- **ME:** It's still NOT FUCKING WORKING
  - **GPT:** As an AI language model...



So unfortunately, I ended up losing a lot of time and emotional bandwidth (which should have been spent on solving the problem) on trying to get an AI to debug its own mess. When I could have just solved it myself the whole time.

![Debugging meme](/assets/img/debugging-meme.jpg){: .mx-auto.d-block :}

You think you are learning, but in reality, you are just setting yourself up for failure. It's quite literally akin to cheating, if someone gave you all the answers to the test and then you read them to "understand" how they got to that answer, you think that's learning? 

We don't use calculators to help us learn math, we use them as assistants to speed up solving mathematical problems that we already know HOW to do. That is the key difference and why people trying to compare this with calculators and other shit are delusional as fuck. You don't learn driving by watching someone drive, if that were the case by the time you were 9 you could drive after watching your mother or father drive you to and from different places. You learn also by doing. 



### Programming IS frustration

Programming is a lot of doing. Programming is frustrating, hard, boring and sometimes it feels "impossible", it was never meant to be straightforward, we are creatives, creative work is never "straight" forward. Hence why the crackhead shit we do is creative ;}. Creativity is chaos. 

If you are feeling all of these things, then that is a good sign, because I feel those things now, and it's an immediate sign that I'm coming to the end of a problem, or beginning to form a solution to a problem. 

Frustration is the bread and butter of programming, it's what builds your character. As clichÃ© as that sounds, I find this to be true. At least in my experiences, being upset has left me coming out a better and better programmer every time, and equips me with new mental tools that I can use to tackle new problems down the line. Using AI tools to alleviate your current frustrations now is just offloading it to be dealt with later, and in a worse manner than it already is now, depending on where the code is, it turns from your issue, to everyone's issue.

You are stunting your growth as a developer and as a human, by having someone else solve your problems for you. Taking responsibility for your errors goes farther than just code. There's no reason for you to need ChatGPT or another AI tool to solve your code for you. 

Embrace the frustration and creative process of devising ways to solve problems, don't view it as a wall or something to get past and delegate to another intelligence, you aren't really "programming" shit if you are doing this.

I must make clear, if it wasn't already clear: I am not against AI, nor am I against the usage of AI in the ASSISTANCE of your development process. What I am against is using it as an excuse and crutch to not try and solve the problem yourself and be frustrated. No, spending 10 minutes on a problem isn't "trying" to solve it. I try to tackle a problem for at least an hour or 2 before offloading my frustration to let's say, someone on StackOverflow or Discord.

I try to make a mental map of the problem, I go back to the drawing board (literally, not figuratively). Something I also like doing is turning off and hiding all distractions, and closing my eyes and sitting for 10 - 15 minutes. The first 5 of those minutes to calm myself down as I usually have heightened emotions, then the rest of those minutes I use to try and think of nothing. I'm essentially meditating, and letting my brain work in [diffuse mode](https://oakland.edu/cetl/teaching-resources/teaching-tips/2019/While-It-Simmers-Engaging-Focused-and-Diffuse-Modes-of-Thinking) to come up with a solution.

Ideally I would have figured it out by then, but some problems are just hard. So it could take a few more hours or even days to fix a persistent issue in a codebase. That's fine, that is programming. But it's 2024 and I generally agree with others that we developers spend a lot of time that we shouldn't spend solving trivial issues. Which is why I understand people turning to ChatGPT to help. But the quickest route is not always the best route (sure someone's said this shit before). 

I will be completely honest though, I use ChatGPT a lot for syntax. Syntax is something I personally make no effort to remember, it's simple enough for ChatGPT to not fuck up (most of the time), and bland enough for me to make an excuse to use AI for it, and having ChatGPT spit out a bunch of different code showcasing how to use some particular syntax is a blessing and time saver, especially when you forget how to do a for loop or access values at array indexes (Yes that is something I forget, leave me alone.). 

#### Learning is frustration

The same way you use language documentation as a reference, is the way you ideally should use ChatGPT. It's funny because I keep this sort of "philosophy" and way of doing things even when it has nothing to do with AI. 

Normally when doing something that's been made before, I'll try and refrain from a tutorial which shows you the implementation of the piece of software I am aiming to make and find something that pushes me in the right direction while ultimately leaving the implementation up to me. Currently I'm working on a [Chip8](https://en.wikipedia.org/wiki/CHIP-8) emulator and I won't lie it's pretty rough because I'm using a technical reference to implement the chip8 instructions, rather than a tutorial. If I used a tutorial I would have been done with it in a day, but that would ultimately have hidden a shit ton of information from me that I found purely from being frustrated and lacking a lot of foundational knowledge in regards to Computer Science. I am not in university yet at the time of writing this, and have no formal education in computer science. 

So there was a ton of "basic" things I didn't know about that I couldn't gloss over because then I wouldn't be able to understand how to decode the instructions from the [ROM](https://en.wikipedia.org/wiki/Read-only_memory) file.

Overall this approach to Chip8 has led me to picking up a book on computer architecture, the first few sections of which start by covering binary digits and logic gates. Which has given me the preliminary knowledge I need to understand how Chip8 works, and if I decide to go on and emulate other hardware like [Gameboy](https://en.wikipedia.org/wiki/Game_Boy) or [NES](https://en.wikipedia.org/wiki/Nintendo_Entertainment_System), I will be able to emulate them without using a tutorial as a crutch.

Nothing is wrong with a tutorial, but as you can see I'm a big advocate of "go through the process yourself, and get frustrated as fuck, repeat". And this is mostly because what I said before regarding my cognitive abilities diminishing from lack of said process.

This might sound like it, but is very different from "reinventing the wheel". You are not inventing anything, you are just trying it yourself before using something that gives you the answer. Tutorials literally give you the answer. This is why people get stuck in what is known as "tutorial hell". Most tutorials teach you to follow and implement solutions that have already been made, but not how to come up with your own solutions. Hence why complete newbies (and me at one point) have issues starting their own project! 
 
Nobody taught you how to solve problems, how to plan or how to design a project. Someone already did that for you, alleviated you of that frustration, that will now propagate itself in the future, aka when you try to program and design your own project. 


#### The fix

The fix I found was: maximize frustration, while minimizing cognitive offloading. That is literally it. If you have found yourself becoming increasingly reliant on AI tools, this is what worked for me. I have to remind myself this everyday because there is a little fucker in my brain that goes "It wouldn't hurt to just ask Mr GPT about it ;], you know, help you ease your frustration.". It feels good in the short term, but in the long term it does no good for you, and not to mention you start feeling like shit. Imposter syndrome is already bad, now imagine even though you are outputting a lot of code (technically the AI is outputting, but whatever.) you know it isn't really you doing all that work. It's one thing to feel like a bad programmer, now you don't even feel like one! You don't get any "Aha! I did it" moments, all of those are given to the AI model you chose to jerk you off.

The point is spend less time trying to evade frustration and stagnation when learning, and indulge in it, that is where I find myself getting the fulfillment and growth.

Resources that push you in the right direction > Resources that give you the directions.

### How I use AI

Currently I only use AI when I'm really fucking stuck, and when I do the way I use AI is by beating around the bush, literally. Instead of straight up asking "How do I implement xyz", I ask incremental questions that will force me to still adapt the code to my codebase and think. I also do not give ChatGPT my code, only the issue I am trying to solve. Let's say I am trying to put a struct inside a struct in Rust (nested structs) for a simple File system.

```rust
struct File {
  name: String,
  size_bytes: u64,
}
```

For this example I will act clueless. I do not know how to do this. So I think for a bit, and let's say I found nothing very useful online. So I go to GPT. The goal is to embed this File struct, with these fields, into a Directory struct which would then simulate having files in a directory.

Instead of showing ChatGPT the code I currently have. I'll simply ask him what I'm trying to do:

![nestedstructs](/assets/img/nested_structs.png){: .mx-auto.d-block :}

Okay cool now we know how to nest structs:

```rust
struct File {
  name: String,
  size_bytes: u64,
}

struct Directory {
  name: String,
  files: File,
}
```

Slight issue though, directories normally hold more than one file. So having the directory only take one file isn't exactly ideal. So I'll ask ChatGPT a follow-up question:

![nestedstructs2](/assets/img/nested_structs2.png){: .mx-auto.d-block :}

Yes I know this is a very basic example, But you could genuinely not know Vectors exist in Rust.

```rust
struct File {
  name: String,
  size_bytes: u64,
}

struct Directory {
  name: String,
  files: Vec<File>,
}
```

This was more of a syntax example (which is normally what I bother ChatGPT for) than an actual problem-solving example, but the same applies for something a bit more nuanced: Beat around the bush and integrate the solution yourself. Try not to ask for a direct solution to your existing code. In fact you could ask ChatGPT to not give you code:

![nestedstructs3](/assets/img/nested_structs3.png){: .mx-auto.d-block :}

But it's also important to realize ChatGPT (And other AI tools) isn't an expert and what he tells you could be very very wrong, which is why I try to stay clear from current AI tools as much as possible. As great as they seem, they are limited in ability and in trustworthiness, this little shit will literally tell you anything that sounds good, including making up code and language features that look really fucking plausible.

### Closing thoughts and prayers

I just hope aspiring programmers don't get gridlocked by AI tools and wind up being lobotomized handicap code gluers. Alright I'm out.



